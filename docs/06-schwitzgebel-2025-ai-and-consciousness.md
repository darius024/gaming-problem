## Citation
- **Title**: AI and Consciousness
- **Author**: Eric Schwitzgebel
- **Venue / year**: Cambridge Elements manuscript (Oct 8, 2025)
- **Link**: `https://faculty.ucr.edu/~eschwitz/SchwitzPapers/AIConsciousness-251008.pdf`
- **Last reviewed**: w01

## One-paragraph takeaway
Schwitzgebel’s central thesis is that **we won’t know** (in time) whether advanced AI systems are conscious before society is forced to make high-stakes decisions about them. A core argument relevant to our project is his **Mimicry Argument**: when a system is trained/designed to **mimic human-like outputs**, the usual inference from “human-like behavior” to “consciousness” is **undercut** (even if not refuted). This frames the core measurement difficulty behind consciousness indicators—and why “passing the test” may not mean what we want it to mean.

## What the author claims (high level)
- **Fog thesis**: plausible futures include both “richly conscious AI soon” and “empty mimicry”; we lack decisive methods and won’t get them before deployment pressure.
- **Against easy dismissals**: it’s not *obvious* AI consciousness is impossible; many serious theories would permit it in principle.
- **Mimicry Argument (Chapter 7)**:
  - define mimicry: surface feature S2 resembles S1 in a model entity because of effects on an observer who treats S1 as indicating some deeper feature F
  - if you know an entity is a mimic, you cannot infer F from S2 the way you would infer F from S1 in the model case
  - applied to AI: systems optimized to produce humanlike text can be “consciousness mimics”, so behavioral evidence alone is weakened

## Relevance to indicator gaming (for digital minds)
- **Indicator(s) involved**: any indicator that leans heavily on **behavioral appearance** (especially verbal self-reports).
- **Primary gaming failure mode(s)**:
  - a model optimized to look conscious to evaluators is exactly the kind of “mimic” the argument warns about
  - “test-passing” can be explained by training for **receiver effects** rather than by underlying conscious structure
- **How this helps our project**: it provides a clean philosophical motivation for why we should expect “indicator gaming” and why demos should be framed as “this evidence is undercut under optimization pressure,” not “this proves non-consciousness.”

## Practical notes for our project
- **How to use this**: in write-ups, explicitly separate:
  - “indicator score / appearance of consciousness” vs
  - “what the indicator was supposed to track”
- **Hardening ideas** (implied):
  - reduce reliance on purely verbal cues
  - prefer tests that require broad generalization or architecture-linked constraints
  - treat claims as probabilistic, with careful attention to training incentives

## Local reference
- Extracted text snapshot: `sources/text/Schwitz_AIConsciousness-251008.txt`

