## Citation
- **Title**: Tests of Animal Consciousness are Tests of Machine Consciousness
- **Author**: Leonard Dung
- **Venue / year**: *Erkenntnis* 90:1323–1342 (published online 2023; journal issue 2025)
- **Link**: `https://link.springer.com/content/pdf/10.1007/s10670-023-00753-9.pdf`
- **Last reviewed**: w01

## One-paragraph takeaway
Dung argues that most proposed positive tests for machine consciousness face the **gaming problem**: we can build “ad hoc / gerrymandered” systems to pass the test while (intuitively) not being conscious. Crucially, he claims this doesn’t make such tests useless. Instead, we should add a **naturalness constraint**: passing a test is evidence only when the capacity wasn’t engineered *specifically* to pass. With that constraint, he argues that the best “theory-light” animal-consciousness methods—**clusters of capacities + masking** and **double dissociation paradigms**—can also serve as useful tests for machine consciousness.

## What the authors claim (high level)
- **Three core claims** (from the abstract):
  - a test can be legitimate even if ad hoc AIs could pass it
  - ad hoc AI can be identified by analogy to **ad hoc hypotheses** in philosophy of science
  - the most reliable animal consciousness tests can be valid positive tests of machine consciousness
- **Gaming problem framing**: when a test targets a “relatively superficial property closely tied to specific behavioral capacities”, designers can often reproduce that capacity without the “complex mechanisms” plausibly linked to experience.
- **Naturalness constraint**: indicator evidence is strongest when success would otherwise be a “strange coincidence” (i.e., not explained by “teaching to the test”).

## Relevance to indicator gaming (for digital minds)
- **Indicator(s) involved**: behavioral/cognitive tests for consciousness (not just verbal self-report).
- **Primary gaming failure mode(s)**:
  - designers can build specialized modules that reproduce a target behavior *just for the test*
  - in AI (and arguably in LLMs), test success can be explained by **mimicry** rather than consciousness
- **How this helps our project**: it provides a clean conceptual vocabulary for write-ups: **ad hoc vs natural**, and a justification for why “gameable” doesn’t mean “worthless”—it means we must model the adversary and the training process.

## Key methods the paper recommends (and why)
- **Theory-light “facilitation hypothesis” (Birch-style)**:
  - look for a **cluster** of cognitive capacities plausibly facilitated by consciousness in humans
  - use **backward masking** to switch conscious access on/off and test whether the cluster covaries with masking
  - crucially: this is only evidential for **non-ad hoc** systems
- **Double dissociation paradigms**:
  - tasks where supraliminal vs subliminal cues have **opposite effects** (a stronger signature than “more stimulus strength → better performance”)
  - combine with the “cluster + masking” approach to reduce “signal strength” confounds

## Practical notes for our project
- **How to use this**:
  - treat our indicator-gaming demo explicitly as constructing (or inducing) an **ad hoc** solution
  - then show why that undermines the indicator’s evidential value under optimization/selection
- **Hardening ideas**:
  - indicators that require broad, *unanticipated* generalization across tasks (harder to gerrymander)
  - test suites built around *clusters* and *cross-task invariants*, not a single metric

## Local reference
- Extracted text snapshot: `sources/text/springer_s10670-023-00753-9.txt`

