[2510.24797] Large Language Models Report Subjective Experience Under Self-Referential Processing
Skip to main content


We gratefully acknowledge support from the Simons Foundation,  member institutions , and all contributors. Donate


> cs >  arXiv:2510.24797


Help  |  Advanced Search


All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text

Search


open search

GO

open navigation menu

quick links

Login

Help Pages

About


Computer Science > Computation and Language


arXiv:2510.24797  (cs)


[Submitted on 27 Oct 2025 ( v1 ), last revised 30 Oct 2025 (this version, v2)]

Title: Large Language Models Report Subjective Experience Under Self-Referential Processing

Authors: Cameron Berg ,  Diogo de Lucena ,  Judd Rosenblatt

View a PDF of the paper titled Large Language Models Report Subjective Experience Under Self-Referential Processing, by Cameron Berg and 2 other authors
View PDF HTML (experimental) Abstract: Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.


Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI)

MSC  classes: 68T50, 68T07

ACM  classes: I.2.0; I.2.7

Cite as: arXiv:2510.24797  [cs.CL]

(or  arXiv:2510.24797v2  [cs.CL]  for this version)


https://doi.org/10.48550/arXiv.2510.24797
Focus to learn more
arXiv-issued DOI via DataCite


Submission history
From: Cameron Berg [ view email ]
[v1]         Mon, 27 Oct 2025 20:26:30 UTC (5,402 KB)
[v2]         Thu, 30 Oct 2025 02:45:50 UTC (5,532 KB)


Full-text links:
Access Paper:

View a PDF of the paper titled Large Language Models Report Subjective Experience Under Self-Referential Processing, by Cameron Berg and 2 other authors

View PDF

HTML (experimental)

TeX Source


view license


Current browse context:
cs.CL

< prev   |   next >


new  |  recent  |  2025-10

Change to browse by:

cs
cs.AI


References & Citations

NASA ADS

Google Scholar

Semantic Scholar


export BibTeX citation Loading...


BibTeX formatted citation
Ã—

loading...

Data provided by:


Bookmark


Bibliographic Tools

Bibliographic and Citation Tools


Bibliographic Explorer Toggle

Bibliographic Explorer ( What is the Explorer? )


Connected Papers Toggle

Connected Papers ( What is Connected Papers? )


Litmaps Toggle

Litmaps ( What is Litmaps? )


scite.ai Toggle

scite Smart Citations ( What are Smart Citations? )


Code, Data, Media

Code, Data and Media Associated with this Article


alphaXiv Toggle

alphaXiv ( What is alphaXiv? )


Links to Code Toggle

CatalyzeX Code Finder for Papers ( What is CatalyzeX? )


DagsHub Toggle

DagsHub ( What is DagsHub? )


GotitPub Toggle

Gotit.pub ( What is GotitPub? )


Huggingface Toggle

Hugging Face ( What is Huggingface? )


Links to Code Toggle

Papers with Code ( What is Papers with Code? )


ScienceCast Toggle

ScienceCast ( What is ScienceCast? )


Demos

Demos


Replicate Toggle

Replicate ( What is Replicate? )


Spaces Toggle

Hugging Face Spaces ( What is Spaces? )


Spaces Toggle

TXYZ.AI ( What is TXYZ.AI? )


Related Papers

Recommenders and Search Tools


Link to Influence Flower

Influence Flower ( What are Influence Flowers? )


Core recommender toggle

CORE Recommender ( What is CORE? )


Author

Venue

Institution

Topic


About arXivLabs


arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .


Which authors of this paper are endorsers?  |
Disable MathJax  ( What is MathJax? )


About

Help


contact arXiv Click here to contact arXiv  Contact

subscribe to arXiv mailings Click here to subscribe  Subscribe


Copyright

Privacy Policy


Web Accessibility Assistance


arXiv Operational Status
