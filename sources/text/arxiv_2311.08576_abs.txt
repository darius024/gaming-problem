[2311.08576] Towards Evaluating AI Systems for Moral Status Using Self-Reports
Skip to main content


We gratefully acknowledge support from the Simons Foundation,  member institutions , and all contributors. Donate


> cs >  arXiv:2311.08576


Help  |  Advanced Search


All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text

Search


open search

GO

open navigation menu

quick links

Login

Help Pages

About


Computer Science > Machine Learning


arXiv:2311.08576  (cs)


[Submitted on 14 Nov 2023]

Title: Towards Evaluating AI Systems for Moral Status Using Self-Reports

Authors: Ethan Perez ,  Robert Long

View a PDF of the paper titled Towards Evaluating AI Systems for Moral Status Using Self-Reports, by Ethan Perez and Robert Long
View PDF Abstract: As AI systems become more advanced and widely deployed, there will likely be increasing debate over whether AI systems could have conscious experiences, desires, or other states of potential moral significance. It is important to inform these discussions with empirical evidence to the extent possible. We argue that under the right circumstances, self-reports, or an AI system's statements about its own internal states, could provide an avenue for investigating whether AI systems have states of moral significance. Self-reports are the main way such states are assessed in humans ("Are you in pain?"), but self-reports from current systems like large language models are spurious for many reasons (e.g. often just reflecting what humans would say). To make self-reports more appropriate for this purpose, we propose to train models to answer many kinds of questions about themselves with known answers, while avoiding or limiting training incentives that bias self-reports. The hope of this approach is that models will develop introspection-like capabilities, and that these capabilities will generalize to questions about states of moral significance. We then propose methods for assessing the extent to which these techniques have succeeded: evaluating self-report consistency across contexts and between similar models, measuring the confidence and resilience of models' self-reports, and using interpretability to corroborate self-reports. We also discuss challenges for our approach, from philosophical difficulties in interpreting self-reports to technical reasons why our proposal might fail. We hope our discussion inspires philosophers and AI researchers to criticize and improve our proposed methodology, as well as to run experiments to test whether self-reports can be made reliable enough to provide information about states of moral significance.


Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

Cite as: arXiv:2311.08576  [cs.LG]

(or  arXiv:2311.08576v1  [cs.LG]  for this version)


https://doi.org/10.48550/arXiv.2311.08576
Focus to learn more
arXiv-issued DOI via DataCite


Submission history
From: Ethan Perez [ view email ]
[v1]         Tue, 14 Nov 2023 22:45:44 UTC (85 KB)


Full-text links:
Access Paper:

View a PDF of the paper titled Towards Evaluating AI Systems for Moral Status Using Self-Reports, by Ethan Perez and Robert Long

View PDF

TeX Source


view license


Current browse context:
cs.LG

< prev   |   next >


new  |  recent  |  2023-11

Change to browse by:

cs
cs.AI
cs.CL


References & Citations

NASA ADS

Google Scholar

Semantic Scholar


export BibTeX citation Loading...


BibTeX formatted citation
Ã—

loading...

Data provided by:


Bookmark


Bibliographic Tools

Bibliographic and Citation Tools


Bibliographic Explorer Toggle

Bibliographic Explorer ( What is the Explorer? )


Connected Papers Toggle

Connected Papers ( What is Connected Papers? )


Litmaps Toggle

Litmaps ( What is Litmaps? )


scite.ai Toggle

scite Smart Citations ( What are Smart Citations? )


Code, Data, Media

Code, Data and Media Associated with this Article


alphaXiv Toggle

alphaXiv ( What is alphaXiv? )


Links to Code Toggle

CatalyzeX Code Finder for Papers ( What is CatalyzeX? )


DagsHub Toggle

DagsHub ( What is DagsHub? )


GotitPub Toggle

Gotit.pub ( What is GotitPub? )


Huggingface Toggle

Hugging Face ( What is Huggingface? )


Links to Code Toggle

Papers with Code ( What is Papers with Code? )


ScienceCast Toggle

ScienceCast ( What is ScienceCast? )


Demos

Demos


Replicate Toggle

Replicate ( What is Replicate? )


Spaces Toggle

Hugging Face Spaces ( What is Spaces? )


Spaces Toggle

TXYZ.AI ( What is TXYZ.AI? )


Related Papers

Recommenders and Search Tools


Link to Influence Flower

Influence Flower ( What are Influence Flowers? )


Core recommender toggle

CORE Recommender ( What is CORE? )


IArxiv recommender toggle

IArxiv Recommender ( What is IArxiv? )


Author

Venue

Institution

Topic


About arXivLabs


arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?  Learn more about arXivLabs .


Which authors of this paper are endorsers?  |
Disable MathJax  ( What is MathJax? )


About

Help


contact arXiv Click here to contact arXiv  Contact

subscribe to arXiv mailings Click here to subscribe  Subscribe


Copyright

Privacy Policy


Web Accessibility Assistance


arXiv Operational Status
